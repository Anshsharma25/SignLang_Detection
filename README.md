# ğŸ¤² Sign Language Detection

Welcome to the **Sign Language Detection** project! This application uses computer vision and deep learning to detect and classify hand gestures for sign language recognition.

---

## ğŸ“– Overview

This project leverages MediaPipe for hand detection and a custom-trained deep learning model to recognize various sign language gestures. It can predict hand gestures and display the corresponding actions or words in real-time. 
---

## âœ¨ Features

- ğŸ–ï¸ Real-time hand tracking and gesture detection
- ğŸ¯ High accuracy with a custom-trained model
- ğŸ“¹ Uses your webcam for live video input
- ğŸ–¥ï¸ User-friendly interface to display predictions

---

## ğŸ› ï¸ Technologies Used

- **Programming Language**: Python ğŸ
- **Libraries**: 
  - OpenCV ğŸ“· (for video processing)
  - MediaPipe ğŸ¤– (for hand tracking)
  - NumPy ğŸ”¢ (for numerical computations)
  - TensorFlow/Keras ğŸ§  (for deep learning model)
- **Model**: Custom-trained neural network

---

## ğŸš€ How to Run the Project

Follow these steps to set up and run the project locally:

### Prerequisites

1. Python 3.7 or higher ğŸ
2. Install the required libraries:
   ```bash
   pip install -r requirements.txt
   ```

### Clone the Repository

```bash
git clone https://github.com/Anshsharma25/SignLang_Detection.git
cd SignLang_Detection
```

### Run the Application

1. Ensure your webcam is connected.
2. Execute the following steps in order:

   - **Step 1**: Run the helper functions for hand detection:
     ```bash
     python function.py
     ```
   - **Step 2**: (Optional) If training is needed, execute the training script:
     ```bash
     python training.py
     ```
   - **Step 3**: Run the gesture recognition system:
     ```bash
     python hand_landmark_prediction.py
     ```

3. The application will open a live video feed. Perform sign language gestures to see predictions in real-time.

---

## ğŸ“‚ Project Structure

```
SignLang_Detection/
â”‚
â”œâ”€â”€ app.py                # Main application file
â”œâ”€â”€ function.py           # Helper functions for hand tracking and keypoint extraction
â”œâ”€â”€ training.py           # Script for training the gesture recognition model
â”œâ”€â”€ hand_landmark_prediction.py # Script for testing hand gesture recognition
â”œâ”€â”€ model.json            # JSON file of the trained model structure
â”œâ”€â”€ model.h5              # Pre-trained model weights
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md             # Project documentation
```

---

## ğŸ¯ Process Workflow

1. **Hand Detection**: Uses MediaPipe to locate hand landmarks in the video feed.
2. **Keypoint Extraction**: Extracts coordinates of 21 hand landmarks.
3. **Gesture Recognition**: Processes the keypoints through a trained deep learning model to predict the gesture.
4. **Result Display**: Outputs the recognized gesture with its accuracy on the video feed.

---

## ğŸ“Š Model Training

The model was trained using a dataset of hand gesture images. Key steps included:
- Data collection for gestures (A, B, C, etc.)
- Preprocessing and augmentation
- Training a sequential neural network

----

## ğŸŒŸ Future Enhancements

- Add more gestures to the recognition system
- Support for multiple hand gestures simultaneously
- Deploy the application as a web-based service

---

## ğŸ‘¨â€ğŸ’» Author

Developed with â¤ï¸ by [Ansh Sharma](https://github.com/Anshsharma25)

---


